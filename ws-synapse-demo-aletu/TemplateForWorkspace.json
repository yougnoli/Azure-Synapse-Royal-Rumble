{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "ws-synapse-demo-aletu"
		},
		"AzureSqlDB_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDB'"
		},
		"ws-synapse-demo-aletu-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ws-synapse-demo-aletu-WorkspaceDefaultSqlServer'"
		},
		"ws-synapse-demo-aletu-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalakedemoaletu.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/GetTablesAndSchemas')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "GetTablesAndSchemas",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "select \n\tname as TableName\n\t,schema_name(schema_id) as SchemaName\nfrom sys.tables\n",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "TablesLabVendite",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "ForEachTableNameANDSchemaName",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "GetTablesAndSchemas",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('GetTablesAndSchemas').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Copy data1",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "@concat('select * from ',item().SchemaName,'.',item().TableName)",
												"type": "Expression"
											},
											"queryTimeout": "02:00:00",
											"partitionOption": "None"
										},
										"sink": {
											"type": "ParquetSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "ParquetWriteSettings"
											}
										},
										"enableStaging": false,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "TablesLabVendite",
											"type": "DatasetReference",
											"parameters": {}
										}
									],
									"outputs": [
										{
											"referenceName": "ParquetLabVendite",
											"type": "DatasetReference",
											"parameters": {
												"folderName": "@item().SchemaName",
												"folderPath": "@item().TableName"
											}
										}
									]
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "lab vendite"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/TablesLabVendite')]",
				"[concat(variables('workspaceId'), '/datasets/ParquetLabVendite')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ParquetLabVendite')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ws-synapse-demo-aletu-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"folderName": {
						"type": "string"
					},
					"folderPath": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@concat(\n    dataset().folderName,'/',\n    dataset().folderPath)",
							"type": "Expression"
						},
						"fileSystem": "lab-vendite"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ws-synapse-demo-aletu-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TablesLabVendite')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDB",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDB')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDB')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDB_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws-synapse-demo-aletu-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('ws-synapse-demo-aletu-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws-synapse-demo-aletu-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ws-synapse-demo-aletu-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00-data-exploration-serverless')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01-serverless"
				},
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakedemoaletu.dfs.core.windows.net/nyc-taxi/NYCTripSmall.parquet',\n        FORMAT='PARQUET'\n    ) AS [result]\n;\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00-load-data')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01-dedicated"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'NYCTaxiTripSmall' AND O.TYPE = 'U' AND S.NAME = 'dbo')\nCREATE TABLE dbo.NYCTaxiTripSmall\n    (\n     [DateID] int,\n     [MedallionID] int,\n     [HackneyLicenseID] int,\n     [PickupTimeID] int,\n     [DropoffTimeID] int,\n     [PickupGeographyID] int,\n     [DropoffGeographyID] int,\n     [PickupLatitude] float,\n     [PickupLongitude] float,\n     [PickupLatLong] nvarchar(4000),\n     [DropoffLatitude] float,\n     [DropoffLongitude] float,\n     [DropoffLatLong] nvarchar(4000),\n     [PassengerCount] int,\n     [TripDurationSeconds] int,\n     [TripDistanceMiles] float,\n     [PaymentType] nvarchar(4000),\n     [FareAmount] numeric(19,4),\n     [SurchargeAmount] numeric(19,4),\n     [TaxAmount] numeric(19,4),\n     [TipAmount] numeric(19,4),\n     [TollsAmount] numeric(19,4),\n     [TotalAmount] numeric(19,4)\n    )\nWITH\n    (\n    DISTRIBUTION = ROUND_ROBIN,\n     CLUSTERED COLUMNSTORE INDEX\n     -- HEAP\n    )\nGO\n\nCOPY INTO dbo.NYCTaxiTripSmall\n(DateID 1, MedallionID 2, HackneyLicenseID 3, PickupTimeID 4, DropoffTimeID 5,\nPickupGeographyID 6, DropoffGeographyID 7, PickupLatitude 8, PickupLongitude 9, \nPickupLatLong 10, DropoffLatitude 11, DropoffLongitude 12, DropoffLatLong 13, \nPassengerCount 14, TripDurationSeconds 15, TripDistanceMiles 16, PaymentType 17, \nFareAmount 18, SurchargeAmount 19, TaxAmount 20, TipAmount 21, TollsAmount 22, \nTotalAmount 23)\nFROM 'https://datalakedemoaletu.dfs.core.windows.net/nyc-taxi/NYCTripSmall.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET'\n    ,MAXERRORS = 0\n    ,IDENTITY_INSERT = 'OFF'\n)",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPOOL1",
						"poolName": "SQLPOOL1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-data-eploration-dedicated')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01-dedicated"
				},
				"content": {
					"query": "SELECT TOP (100) [DateID]\n,[MedallionID]\n,[HackneyLicenseID]\n,[PickupTimeID]\n,[DropoffTimeID]\n,[PickupGeographyID]\n,[DropoffGeographyID]\n,[PickupLatitude]\n,[PickupLongitude]\n,[PickupLatLong]\n,[DropoffLatitude]\n,[DropoffLongitude]\n,[DropoffLatLong]\n,[PassengerCount]\n,[TripDurationSeconds]\n,[TripDistanceMiles]\n,[PaymentType]\n,[FareAmount]\n,[SurchargeAmount]\n,[TaxAmount]\n,[TipAmount]\n,[TollsAmount]\n,[TotalAmount]\n FROM [dbo].[NYCTaxiTripSmall]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPOOL1",
						"poolName": "SQLPOOL1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-data-exploration-database')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01-serverless"
				},
				"content": {
					"query": "CREATE DATABASE DataExplorationDB \n                COLLATE Latin1_General_100_BIN2_UTF8\n;\nGO\n\nUSE DataExplorationDB\n;\nGO\n\nCREATE EXTERNAL DATA SOURCE DemoLake\nWITH ( LOCATION = 'https://datalakedemoaletu.dfs.core.windows.net')\n;\nGO\n\nCREATE LOGIN data_explorer WITH PASSWORD = 'SeptemberTango2022!'\n;\nGO\n\nCREATE USER data_explorer FOR LOGIN data_explorer\n;\nGO\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO data_explorer\n;\nGO\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n            BULK '/nyc-taxi/NYCTripSmall.parquet',\n            DATA_SOURCE = 'DemoLake',\n            FORMAT='PARQUET'\n    ) AS [result]\n;\nGO\n\n\n\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02-querying-files-serverless')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01-serverless"
				},
				"content": {
					"query": "USE DataExplorationDB\n;\nGO\n\n/*\nTo run queries using serverless SQL pool, create data source that serverless SQL pool can use use to access files in storage.\nA database scoped credential is a record that contains the authentication information that is required to connect to a resource outside SQL Server. \nMost credentials include a Windows user and password.\nBefore creating a database scoped credential, the database must have a master key to protect the credential.\n*/\n\n-- create master key that will protect the credentials:\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'type a password'\n\n-- create credentials for containers in our demo storage account\nCREATE DATABASE SCOPED CREDENTIAL sqlondemand\nWITH IDENTITY='SHARED ACCESS SIGNATURE',  \nSECRET = 'sv=2018-03-28&ss=bf&srt=sco&sp=rl&st=2019-10-14T12%3A10%3A25Z&se=2061-12-31T12%3A10%3A00Z&sig=KlSU2ullCscyTS0An0nozEpo4tO5JAgGBvw%2FJX2lguw%3D'\nGO\nCREATE EXTERNAL DATA SOURCE SqlOnDemandDemo WITH (\n    LOCATION = 'https://sqlondemandstorage.blob.core.windows.net',\n    CREDENTIAL = sqlondemand\n);\n\n-- QUERY CSV FILES\n-- The following query shows how to read a CSV file that doesn't contain a header row, with Windows-style new line, and comma-delimited columns:\nSELECT TOP 10 *\nFROM OPENROWSET\n  (\n      BULK 'csv/population/*.csv',\n      DATA_SOURCE = 'SqlOnDemandDemo',\n      FORMAT = 'CSV', PARSER_VERSION = '2.0'\n  )\nWITH\n  (\n      country_code VARCHAR (5)\n    , country_name VARCHAR (100)\n    , year smallint\n    , population bigint\n  ) AS r\nWHERE\n  country_name = 'Luxembourg' AND year = 2017\n;\nGO\n\n-- QUERY PARQUET FILES\nSELECT COUNT_BIG(*) as num_rows\nFROM OPENROWSET\n  (\n      BULK 'parquet/taxi/year=2017/month=9/*.parquet',\n      DATA_SOURCE = 'SqlOnDemandDemo',\n      FORMAT='PARQUET'\n  ) AS nyc\n;\nGO\n\n-- QUERY JSON FILES\nSELECT\n    JSON_VALUE(jsonContent, '$.title') AS title\n  , JSON_VALUE(jsonContent, '$.publisher') as publisher\n  , jsonContent\nFROM OPENROWSET\n  (\n      BULK 'json/books/*.json',\n      DATA_SOURCE = 'SqlOnDemandDemo'\n    , FORMAT='CSV'\n    , FIELDTERMINATOR ='0x0b'\n    , FIELDQUOTE = '0x0b'\n    , ROWTERMINATOR = '0x0b'\n  )\nWITH\n  ( jsonContent varchar(8000) ) AS [r]\nWHERE\n  JSON_VALUE(jsonContent, '$.title') = 'Probabilistic and Statistical Methods in Cryptology, An Introduction by Selected Topics'\n;\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03-explore-analyze-data')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01-serverless"
				},
				"content": {
					"query": "/*\nFull tutorial available on: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/tutorial-data-analyst\nIn this tutorial, you learn how to perform exploratory data analysis by combining different Azure Open Datasets \nusing serverless SQL pool and then visualizing the results in Azure Synapse Studio.\n\nIn particular, you analyze the New York City (NYC) Taxi dataset that includes:\n\n - Pickup and drop-off dates and times.\n - Pick up and drop-off locations.\n - Trip distances.\n - Itemized fares.\n - Rate types.\n - Payment types.\n - Driver-reported passenger counts.*/\n\n\n/*\n * * * * * * * * * * * * * * * *\n * Automatic schema inference  *\n * * * * * * * * * * * * * * * *\n\nSince data is stored in the Parquet file format, automatic schema inference is available. You can easily query the \ndata without listing the data types of all columns in the files. You also can use the virtual column mechanism and \nthe filepath function to filter out a certain subset of files.\n\nLet's first get familiar with the NYC Taxi data by running the following query. */\n\nselect top 100\n\t*\nfrom\t\n\topenrowset(\n\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet'\n\t\t,format='parquet'\n\t) as nyc\n;\ngo\n\nselect\n\tcount(*) as num_rows\nfrom\t\n\topenrowset(\n\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet'\n\t\t,format='parquet'\n\t) as nyc\n;\ngo -- 1,571,671,152\n\n/* Similarly, you can query the Public Holidays dataset by using the following query. */\n\nselect top 100\n\t*\nfrom\t\n\topenrowset(\n\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/holidaydatacontainer/Processed/*.parquet'\n\t\t,format='parquet'\n\t) as holidays\nwhere\n\tcountryOrRegion = 'italy' and year([date]) = 2016\n;\ngo\n\n/* Lastly, you can also query the Weather Data dataset by using the following query. */\n\nselect top 100\n\t*\nfrom\t\n\topenrowset(\n\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/year=*/month=*/*.parquet'\n\t\t,format='parquet'\n\t) as weather\n;\ngo\n\n/*\n * * * * * * * * * * * * * * * * * * * * * * * * * *\n * Time series, seasonality, and outlier analysis  *\n * * * * * * * * * * * * * * * * * * * * * * * * * *\nYou can easily summarize the yearly number of taxi rides by using the following query. */\n\nselect\n    year(tpepPickupDateTime) AS current_year\n    ,count(*) as rides_per_year\nfrom\t\n\topenrowset(\n\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet'\n\t\t,format='parquet'\n\t) as nyc\nwhere \n\tnyc.filepath(1) >= '2009' AND nyc.filepath(1) <= '2019'\ngroup by \n\tyear(tpepPickupDateTime)\norder by \n\tyear(tpepPickupDateTime)\n;\ngo\n\n/* The data can be visualized in Synapse Studio by switching from the Table to the Chart view.\nYou can choose among different chart types, such as Area, Bar, Column, Line, Pie, and Scatter.\nIn this case, plot the Column chart with the Category column set to current_year.\n\nFrom this visualization, a trend of a decreasing number of rides over years can be clearly seen.\nPresumably, this decrease is due to the recent increased popularity of ride-sharing companies.\n*/\n\n/* Next, let's focus the analysis on a single year, for example, 2016.\nThe following query returns the daily number of rides during that year. */\n\nselect\n    convert(date, [tpepPickupDateTime]) as [current_day]\n    ,count(*) as rides_per_day\nfrom\t\n\topenrowset(\n\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet'\n\t\t,format='parquet'\n\t) as nyc\nwhere \n\tnyc.filepath(1) = '2016'\ngroup by \n\tconvert(date, [tpepPickupDateTime])\norder by \n\tconvert(date, [tpepPickupDateTime])\n;\ngo\n\n/* Again, you can easily visualize data by plotting the Column chart with\nthe Category column set to current_day and the Legend (series) column set to rides_per_day. */\n\n/* From the plot chart, you can see that there's a weekly pattern, with Saturdays as the peak day.\nDuring summer months, there are fewer taxi rides because of vacations.\nThere are also some significant drops in the number of taxi rides without a clear pattern of when and why they occur. */\n\n/* Next, let's see if the drops correlate with public holidays by joining the NYC Taxi rides dataset with the Public Holidays dataset. */\n\nwith taxi_rides as\n(\n\tselect\n\t\tconvert(date, [tpepPickupDateTime]) as [current_day]\n\t\t,count(*) as rides_per_day\n\tfrom\t\n\t\topenrowset(\n\t\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=*/puMonth=*/*.parquet'\n\t\t\t,format='parquet'\n\t\t) as nyc\n\twhere \n\t\tnyc.filepath(1) = '2016'\n\tgroup by \n\t\tconvert(date, [tpepPickupDateTime])\n),\npublic_holidays AS\n(\n    select\n        holidayname as holiday\n        ,convert(date, [date]) as [date]\n\tfrom\t\n\t\topenrowset(\n\t\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/holidaydatacontainer/Processed/*.parquet'\n\t\t\t,format='parquet'\n\t\t) as holidays\n\twhere\n\t\tcountryorregion = 'United States' and year([date]) = 2016\n)\nselect\n\t*\nfrom \n\ttaxi_rides as a\n\tleft outer join\n\tpublic_holidays as b \n\ton a.current_day = b.[date]\norder by \n\tcurrent_day\n;\ngo\n\n/* This time, we want to highlight the number of taxi rides during public holidays.\nFor that purpose, we choose none for the Category column and rides_per_day and holiday as the Legend (series) columns. */\n\n/* From the plot chart, you can see that during public holidays the number of taxi rides is lower.\nThere's still one unexplained large drop on January 23. Let's check the weather in NYC on that day by querying the Weather Data dataset. */\n\nselect\n\t*\nfrom\n\topenrowset(\n\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/year=*/month=*/*.parquet'\n\t\t,format='parquet'\n\t) as weather\nwhere \n\tcountryorregion = 'US' and convert(date, [datetime]) = '2016-01-23' and stationname = 'JOHN F KENNEDY INTERNATIONAL AIRPORT'\n;\ngo\n\nselect\n    avg(windspeed)\t\tas avg_windspeed\n    ,min(windspeed)\t\tas min_windspeed\n    ,max(windspeed)\t\tas max_windspeed\n    ,avg(temperature)\tas avg_temperature\n    ,min(temperature)\tas min_temperature\n    ,max(temperature)\tas max_temperature\n    ,avg(sealvlpressure) as avg_sealvlpressure\n    ,min(sealvlpressure) as min_sealvlpressure\n    ,max(sealvlpressure) as max_sealvlpressure\n    ,avg(precipdepth)\tas avg_precipdepth\n    ,min(precipdepth)\tas min_precipdepth\n    ,max(precipdepth)\tas max_precipdepth\n    ,avg(snowdepth)\t\tas avg_snowdepth\n    ,min(snowdepth)\t\tas min_snowdepth\n    ,max(snowdepth)\t\tas max_snowdepth\nfrom\n\topenrowset(\n\t\tbulk 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather/year=*/month=*/*.parquet'\n\t\t,format='parquet'\n\t) as weather\nwhere \n\tcountryorregion = 'US' and convert(date, [datetime]) = '2016-01-23' and stationname = 'JOHN F KENNEDY INTERNATIONAL AIRPORT'\n;\ngo\n\n/* The results of the query indicate that the drop in the number of taxi rides occurred because:\n\n1. There was a blizzard on that day in NYC with heavy snow (~30 cm).\n2. It was cold (temperature was below zero degrees Celsius).\n3. It was windy (~10 m/s). */\n\n\n/* This tutorial has shown how a data analyst can quickly perform exploratory data analysis, easily combine different\ndatasets by using serverless SQL pool, and visualize the results by using Azure Synapse Studio. */",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/user-porinicorsi01')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- get tables from porinicorsi01 LabVendite\n\nselect * from sys.sysusers\n\n-- creo una SQL login -> mi posiziono sul master:\ncreate login syn_reader with password = 'AlfaBravo22$'\t--> questa non può fare assolutamente nulla! Neanche connettersi\n\n-- creo USER (nota: il nome dello user può essere diverso dalla login => sono 2 oggetti diversi) -> mi posiziono sul master:\ncreate user syn_reader from login syn_reader\n\n-- mi posiziono sul db Test\n-- creo USER (nota: il nome dello user può essere diverso dalla login => sono 2 oggetti diversi):\ncreate user syn_reader from login syn_reader\n\n-- Voglio dare i diritti a syn_reader di leggere qualsiasi cosa (fare una SELECT su tutte le tabelle):\nexec sp_addrolemember 'db_datareader', 'syn_reader'\n\n-- CAMBIO USER:\nexecute as user = 'syn_reader'\n-- TORNO ALLA MIA UTENZA INIZIALE\nrevert\n-- CONTROLLO UTENZA\nselect user_name () as user_now, original_login() as user_original\n\nselect top 100 * from dbo.negozi\n\nselect \n\tname as tablename\n\t,schema_name(schema_id) as schemaname\nfrom sys.tables",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lakehouse-giac')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool01",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "72b0d95f-c1d5-4532-9237-1eabb93dd4dc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/2456bd8c-7ce3-4a81-9a96-847fe17f01f2/resourceGroups/tugnoli-synapse-aletu/providers/Microsoft.Synapse/workspaces/ws-synapse-demo-aletu/bigDataPools/SparkPool01",
						"name": "SparkPool01",
						"type": "Spark",
						"endpoint": "https://ws-synapse-demo-aletu.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool01",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Caricare i dati iniziali dalla cartella delle vendite dentro a un dataframe"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"sales_df = spark.read.load('abfss://lab-vendite@datalakedemoaletu.dfs.core.windows.net/stg/sales', format='parquet')\r\n",
							"display(sales_df.limit(10))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"*Qua ci dovrebbe essere una fase di data cleansing o enriching* "
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Faccio diventare il dataframe in una tabella Delta"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"(\r\n",
							"    sales_df\r\n",
							"        .write\r\n",
							"        .format('delta')\r\n",
							"        .mode('overwrite')\r\n",
							"        .save('abfss://lakehouse-silver@datalakedemoaletu.dfs.core.windows.net/sale')\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Far diventare un dataframe in una tempview"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sales_df.createOrReplaceTempView('FactSale')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"select * \r\n",
							"from FactSale\r\n",
							"limit 10"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Posso usare questa tempview per scrivere un database"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DROP TABLE IF EXISTS LabVendite.FactSale;\r\n",
							"\r\n",
							"DROP DATABASE IF EXISTS LabVendite;\r\n",
							"\r\n",
							"CREATE DATABASE LabVendite;\r\n",
							"\r\n",
							"CREATE TABLE LabVendite.FactSale\r\n",
							"USING DELTA\r\n",
							"LOCATION 'abfss://lakehouse-silver@datalakedemoaletu.dfs.core.windows.net/LabVendite/Fact'\r\n",
							"        AS SELECT * FROM FactSale -- tempview"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*)\r\n",
							"from labvendite.factsale"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select *\r\n",
							"from labvendite.factsale\r\n",
							"limit 10"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"deltaPath = 'abfss://lakehouse-silver@datalakedemoaletu.dfs.core.windows.net/LabVendite/Fact'\r\n",
							"deltaTable = DeltaTable.forPath(spark, deltaPath)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"deltaTable_df = deltaTable.toDF()\r\n",
							"display(deltaTable_df.limit(10))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(deltaTable.history())"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"deltaTable.delete(\"Importo = 0\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"(spark.read.format(\"delta\")\r\n",
							".option(\"versionAsOf\", \"0\")\r\n",
							".load(deltaPath).count())"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"(spark.read.format(\"delta\")\r\n",
							".option(\"versionAsOf\", \"1\")\r\n",
							".load(deltaPath).count())"
						],
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkPool01')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "northeurope"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLPOOL1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "northeurope"
		}
	]
}